{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#WorkFlow\n",
        "\n",
        "simple (vanilla) Gradient Descent:\n",
        "\n",
        "computed the forward pass (Y_hat = W*X + B).\n",
        "\n",
        "Calculated the loss (Mean Squared Error).\n",
        "\n",
        "Computed gradients manually (dW and dB).\n",
        "\n",
        "Updated the parameters using gradient descent rule (W_new = W - lr * dW, B_new = B - lr* dB).\n",
        "\n",
        "Repeated for multiple iterations."
      ],
      "metadata": {
        "id": "Ja_AItnKUYgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1 (without early stopping)"
      ],
      "metadata": {
        "id": "q026EVG1bRWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AghpgKZT9IF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Initialize data points\n",
        "X = np.array([1, 2, 3, 4, 5, 6], dtype=int)    # inputs - We can just change the values\n",
        "Y = np.array([5, 8, 11, 14, 17, 20], dtype=int) # true labels - We can just change the values\n",
        "\n",
        "# Step 2: Initialize parameters\n",
        "W = 0.0\n",
        "B = 0.0\n",
        "learning_rate = 0.01\n",
        "n_iterations = 50\n",
        "n = len(X)\n",
        "\n",
        "# Gradient Descent Loop\n",
        "for i in range(n_iterations):\n",
        "    print(f\"\\n================ Iteration {i+1} ================\")\n",
        "\n",
        "    # Step 1: Forward pass and error\n",
        "    print(\"Step 1: Forward Pass and Error Computation\")\n",
        "    print(\" X | Y  | Y_hat = W*X + B | Error = Y_hat - Y | dW_contrib = Error*X | dB_contrib = Error\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    Y_hat_list = []\n",
        "    Error_list = []\n",
        "    dW_contrib_list = []\n",
        "    dB_contrib_list = []\n",
        "\n",
        "    for xi, yi in zip(X, Y):\n",
        "        y_hat = W * xi + B\n",
        "        error = y_hat - yi\n",
        "        dw_contrib = error * xi\n",
        "        db_contrib = error\n",
        "\n",
        "        Y_hat_list.append(y_hat)\n",
        "        Error_list.append(error)\n",
        "        dW_contrib_list.append(dw_contrib)\n",
        "        dB_contrib_list.append(db_contrib)\n",
        "\n",
        "        # Print each calculation as an equation\n",
        "        print(f\"{xi:>2} | {yi:>2} | {y_hat:>6.2f} = {W:.2f}*{xi} + {B:.2f} | \"\n",
        "              f\"{error:>6.2f} = {y_hat:.2f} - {yi} | {dw_contrib:>10.2f} = {error:.2f}*{xi} | {db_contrib:>9.2f} = {error:.2f}\")\n",
        "\n",
        "    # Step 2: Compute average gradients\n",
        "    dW = (1/n) * np.sum(dW_contrib_list)\n",
        "    dB = (1/n) * np.sum(dB_contrib_list)\n",
        "\n",
        "    print(\"\\nStep 2: Compute Average Gradients\")\n",
        "    print(f\"dW = (1/n) * sum(dW_contrib) = (1/{n}) * {np.sum(dW_contrib_list):.2f} = {dW:.4f}\")\n",
        "    print(f\"dB = (1/n) * sum(dB_contrib) = (1/{n}) * {np.sum(dB_contrib_list):.2f} = {dB:.4f}\")\n",
        "\n",
        "    # Step 3: Update parameters\n",
        "    W_new = W - learning_rate * dW\n",
        "    B_new = B - learning_rate * dB\n",
        "    print(\"\\nStep 3: Update Parameters\")\n",
        "    print(f\"W_new = W - lr*dW = {W:.4f} - {learning_rate}*{dW:.4f} = {W_new:.4f}\")\n",
        "    print(f\"B_new = B - lr*dB = {B:.4f} - {learning_rate}*{dB:.4f} = {B_new:.4f}\")\n",
        "\n",
        "    # Step 4: Compute loss\n",
        "    Error_array = np.array(Error_list)\n",
        "    loss = (1/(2*n)) * np.sum(Error_array**2)\n",
        "    print(f\"\\nStep 4: Compute Loss\")\n",
        "    print(f\"L = 1/(2n) * sum(Error^2) = 1/(2*{n}) * {np.sum(Error_array**2):.2f} = {loss:.4f}\")\n",
        "\n",
        "    # Update parameters for next iteration\n",
        "    W = W_new\n",
        "    B = B_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2 (with early stopping)"
      ],
      "metadata": {
        "id": "Jyy6ESIbbWBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Initialize data points\n",
        "X = np.array([1, 2, 3, 4, 5, 6], dtype=int)    # inputs - We can just change the values\n",
        "Y = np.array([5, 8, 11, 14, 17, 20], dtype=int) # true labels - We can just change the values\n",
        "\n",
        "# Step 2: Initialize parameters\n",
        "W = 0\n",
        "B = 0\n",
        "learning_rate = 0.01\n",
        "n_iterations = 10000  # Set a very high number; early stopping will break before reaching this\n",
        "n = len(X)\n",
        "tolerance = 1e-6      # Early stopping tolerance\n",
        "prev_loss = float('inf')\n",
        "\n",
        "# Open a file to save full log\n",
        "log_file = open(\"gradient_descent_log.txt\", \"w\")\n",
        "\n",
        "# Gradient Descent Loop\n",
        "for i in range(n_iterations):\n",
        "    print(f\"\\n================ Iteration {i+1} ================\")\n",
        "    log_file.write(f\"\\n================ Iteration {i+1} ================\\n\")\n",
        "\n",
        "    # Step 1: Forward pass and error\n",
        "    print(\"Step 1: Forward Pass and Error Computation\")\n",
        "    log_file.write(\"Step 1: Forward Pass and Error Computation\\n\")\n",
        "    print(\" X | Y  | Y_hat = W*X + B | Error = Y_hat - Y | dW_contrib = Error*X | dB_contrib = Error\")\n",
        "    log_file.write(\" X | Y  | Y_hat = W*X + B | Error = Y_hat - Y | dW_contrib = Error*X | dB_contrib = Error\\n\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    log_file.write(\"--------------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "    Y_hat_list = []\n",
        "    Error_list = []\n",
        "    dW_contrib_list = []\n",
        "    dB_contrib_list = []\n",
        "\n",
        "    for xi, yi in zip(X, Y):\n",
        "        y_hat = W * xi + B\n",
        "        error = y_hat - yi\n",
        "        dw_contrib = error * xi\n",
        "        db_contrib = error\n",
        "\n",
        "        Y_hat_list.append(y_hat)\n",
        "        Error_list.append(error)\n",
        "        dW_contrib_list.append(dw_contrib)\n",
        "        dB_contrib_list.append(db_contrib)\n",
        "\n",
        "        # Print each calculation as an equation\n",
        "        line = (f\"{xi:>2} | {yi:>2} | {y_hat:>6.2f} = {W:.2f}*{xi} + {B:.2f} | \"\n",
        "                f\"{error:>6.2f} = {y_hat:.2f} - {yi} | {dw_contrib:>10.2f} = {error:.2f}*{xi} | {db_contrib:>9.2f} = {error:.2f}\")\n",
        "        print(line)\n",
        "        log_file.write(line + \"\\n\")\n",
        "\n",
        "    # Step 2: Compute average gradients\n",
        "    dW = (1/n) * np.sum(dW_contrib_list)\n",
        "    dB = (1/n) * np.sum(dB_contrib_list)\n",
        "\n",
        "    print(\"\\nStep 2: Compute Average Gradients\")\n",
        "    log_file.write(\"\\nStep 2: Compute Average Gradients\\n\")\n",
        "    print(f\"dW = (1/n) * sum(dW_contrib) = (1/{n}) * {np.sum(dW_contrib_list):.2f} = {dW:.4f}\")\n",
        "    log_file.write(f\"dW = (1/n) * sum(dW_contrib) = (1/{n}) * {np.sum(dW_contrib_list):.2f} = {dW:.4f}\\n\")\n",
        "    print(f\"dB = (1/n) * sum(dB_contrib) = (1/{n}) * {np.sum(dB_contrib_list):.2f} = {dB:.4f}\")\n",
        "    log_file.write(f\"dB = (1/n) * sum(dB_contrib) = (1/{n}) * {np.sum(dB_contrib_list):.2f} = {dB:.4f}\\n\")\n",
        "\n",
        "    # Step 3: Update parameters\n",
        "    W_new = W - learning_rate * dW\n",
        "    B_new = B - learning_rate * dB\n",
        "    print(\"\\nStep 3: Update Parameters\")\n",
        "    log_file.write(\"\\nStep 3: Update Parameters\\n\")\n",
        "    line = f\"W_new = W - lr*dW = {W:.4f} - {learning_rate}*{dW:.4f} = {W_new:.4f}\"\n",
        "    print(line)\n",
        "    log_file.write(line + \"\\n\")\n",
        "    line = f\"B_new = B - lr*dB = {B:.4f} - {learning_rate}*{dB:.4f} = {B_new:.4f}\"\n",
        "    print(line)\n",
        "    log_file.write(line + \"\\n\")\n",
        "\n",
        "    # Step 4: Compute loss\n",
        "    Error_array = np.array(Error_list)\n",
        "    loss = (1/(2*n)) * np.sum(Error_array**2)\n",
        "    print(f\"\\nStep 4: Compute Loss\")\n",
        "    log_file.write(\"\\nStep 4: Compute Loss\\n\")\n",
        "    line = f\"L = 1/(2n) * sum(Error^2) = 1/(2*{n}) * {np.sum(Error_array**2):.2f} = {loss:.6f}\"\n",
        "    print(line)\n",
        "    log_file.write(line + \"\\n\")\n",
        "\n",
        "    # Step 5: Early stopping\n",
        "    if abs(prev_loss - loss) < tolerance:\n",
        "        print(f\"\\nEARLY STOPPING at iteration {i+1} — loss improvement < {tolerance}\")\n",
        "        log_file.write(f\"\\nEARLY STOPPING at iteration {i+1} — loss improvement < {tolerance}\\n\")\n",
        "        break\n",
        "\n",
        "    prev_loss = loss\n",
        "\n",
        "    # Update parameters for next iteration\n",
        "    W = W_new\n",
        "    B = B_new\n",
        "\n",
        "final_line = f\"\\nOptimal Parameters Found: W = {W:.4f}, B = {B:.4f}, Loss = {loss:.6f}\"\n",
        "print(final_line)\n",
        "log_file.write(final_line + \"\\n\")\n",
        "\n",
        "log_file.close()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_oVFOD7eZGoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"gradient_descent_log.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XOyuljLdapjV",
        "outputId": "4606a489-a8b8-40fa-96b5-b1bcad92c14b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_02df2f9d-0499-413b-890f-2b2e307ae5b7\", \"gradient_descent_log.txt\", 2069687)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}